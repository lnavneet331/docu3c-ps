{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Problem Statement # 1 :  Getting ready to appear for a Trial</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install PyPDF2\n",
    "!pip install openai\n",
    "!pip install nltk\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import openai\n",
    "import os\n",
    "import re\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load api key from .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key=os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  3.9.0\n",
      "re:  2.2.1\n",
      "nltk:  3.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lnavn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print('Python: ', platform.python_version())\n",
    "print('re: ', re.__version__)\n",
    "print('nltk: ', nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the PDF document\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7500\n"
     ]
    }
   ],
   "source": [
    "text = read_pdf(\"ps1.pdf\")\n",
    "token_count = count_tokens(text)\n",
    "print(f\"Number of tokens: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_up_file(tokens, chunk_size, overlap_size):\n",
    "    if len(tokens) <= chunk_size:\n",
    "        yield tokens\n",
    "    else:\n",
    "        chunk = tokens[:chunk_size]\n",
    "        yield chunk\n",
    "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
    "\n",
    "def break_up_file_to_chunks(text, chunk_size=2000, overlap_size=100):\n",
    "    tokens = word_tokenize(text)\n",
    "    return list(break_up_file(tokens, chunk_size, overlap_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: 2000 tokens\n",
      "Chunk 1: 2000 tokens\n",
      "Chunk 2: 2000 tokens\n",
      "Chunk 3: 1800 tokens\n"
     ]
    }
   ],
   "source": [
    "chunks = break_up_file_to_chunks(text)\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i}: {len(chunk)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_detokenized_text(tokenized_text):\n",
    "    prompt_text = \" \".join(tokenized_text)\n",
    "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
    "    return prompt_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_response = []\n",
    "chunks = break_up_file_to_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    prompt_request = \"Please generate a concise summary for the following text:\\n\" + convert_to_detokenized_text(chunks[i])\n",
    "    response = openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=prompt_request,\n",
    "            temperature=.5,\n",
    "            max_tokens=500,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0\n",
    "    )\n",
    "    \n",
    "    prompt_response.append(response[\"choices\"][0][\"text\"].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_request = \"Give the 10 most important items pertaining this trial: \" + str(prompt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.Completion.create(\n",
    "        model=\"text-davinci-003\",\n",
    "        prompt=prompt_request,\n",
    "        temperature=.5,\n",
    "        max_tokens=1000,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.\n",
      "FDA’s approval of mifepristone for wide-scale use\n",
      "2.\n",
      "FDA’s claim that pregnancy is a serious or life-threatening illness\n",
      "3.\n",
      "FDA’s actions that defy federal criminal law\n",
      "4.\n",
      "FDA’s actions that seek to override the balance struck by states\n",
      "5.\n",
      "FDA’s imposition of a federal mail-order abortion regime\n",
      "6.\n",
      "FDA’s actions that make no claim of democratic legitimacy\n",
      "7.\n",
      "FDA’s actions that will undermine States’ ability to protect their citizens\n",
      "8.\n",
      "FDA’s actions that divert scarce resources to investigating and prosecuting violations of their laws\n",
      "9.\n",
      "Harm to the public interest caused by the FDA’s actions\n",
      "10.\n",
      "Amici States entitled to enforce their duly enacted laws\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "meeting_summary = response[\"choices\"][0][\"text\"].strip()\n",
    "\n",
    "# Split the meeting summary into sentences\n",
    "sentences = sent_tokenize(meeting_summary)\n",
    "\n",
    "# Print the sentences with bullet points\n",
    "for i, sentence in enumerate(sentences, start=1):\n",
    "    sentence = sentence.strip()\n",
    "    if sentence != \"\":\n",
    "        print(f\"{sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "femo1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
